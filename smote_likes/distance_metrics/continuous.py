#!/usr/bin/env python3

# Authors: Lyubomir Danov <->
# License: -

import numpy
from sklearn.metrics import euclidean_distances

from .categorical import get_cond_probas
from .helpers import (ShapeError, _discretize_column, _get_all_interval_widths,
                      _get_interpolated_location,
                      _get_interpolated_probability, _remap_ndarray_dict,
                      _update_x_range)


def interpolated_vdm(X: numpy.ndarray, y: numpy.ndarray, s: int) -> numpy.ndarray:
    r"""Computes Interpolated Value Difference Metric

    Parameters
    ----------
    X : numpy.ndarray
        Feature matrix with only categorical features with dimensions (observations, features).
    y : numpy.ndarray
        Target class for each row in X.
    s : int
        Number of categories to group all elements of a feature into.
        Needs to be larger than 0.

    Returns
    -------
    numpy.ndarray
        A distance array of size (observations, observations) with all
        pairwise distances between observations. See notes for returned
        value.

    Notes
    -----
    The distance matrix returned corresponds to the categorical part of equation 22 
    from :cite:t:`Wilson1997`:

    .. math::

        interpolated\_vdm(x, y) \\
        &= \sum_{b=1}^{cat} \sum_{c=1}^{C} \left | p_{b,c}(x) - p_{b,c}(y) \right | ^2

    where C is the list of classes and cat is the list of 
    categorical attributes. 

    The interpolated probability value :math:`p_{a,c}(x)` is taken from equation 23 from :cite:t:`Wilson1997`.

    .. math::

        p_{a,c}(x) &= P_{a,u,c} + z(x) * (P_{a,u+1,c} - P_{a,u,c}) \\
        z(x) &= \left ( \frac {x - mid_{a, u}} {mid_{a, u+1} - mid_{a, u}} \right) \\
        mid_{a, u} &= min_a + width_a * (u + 0.5)

    where:  

    - u is the range in which a discretized x falls (see :py:func:`smote_likes.distance_metrics.discretize_columns`)
    - a is the attribute and c is the class. 

    Note1: The algorithm expects that all attributes of X are categorical features
    encoded as numeric.

    Note2: The returned distance matrix can be summed element-wise 
    with the distance matrices generated by other types of attributes.

    Note3: If :math:`x < mid_{a,u}` then :math:`u_{x} = u_{x} - 1`

    """
    # TODO: tests

    # The value of u is found by first setting u = discretize_{a}(x), and then
    # subtracting 1 from u if x < mid_{a,u}
    # Condition is enforced to guarantee that following holds for further calculation:
    # mid_{a, u} <= x <= mid_{a, u+1}

    # calculate the initial ranges of each entry and the range widths
    X_discrete_init = discretize_columns(X, s)
    widths = _get_all_interval_widths(X, s)
    # calculate the mid points of each range
    # update the range of each entry if condition
    X_discrete = _update_x_range(X_discrete_init, widths)

    cond_proba_list = get_cond_probas(X=X_discrete, y=y)
    unique_targets = numpy.unique(y)

    # get class conditional for each a,u,c
    # for each class
    # generate matrix of size X P_a,u,c
    # generate matrix of size X P_a,u+1,c
    # generate matrix of size X z(x)
    # calculate matrix p_a,c(x)
    # get euclidean_distance

    list_pdist_per_class = []
    for trgt in unique_targets:
        X_p_c_all = []
        for col in range(X_discrete.shape[1]):
            z_a = _get_interpolated_location(X[:, col],
                                             X_discrete[:, col],
                                             widths[0, col],
                                             widths[2, col])
            P_a_u0_c = _remap_ndarray_dict(X_discrete[:, col],
                                           cond_proba_list[col][trgt])
            P_a_u1_c = _remap_ndarray_dict(X_discrete[:, col] + 1,
                                           cond_proba_list[col][trgt])
            X_a_p_c = _get_interpolated_probability(P_a_u0_c, P_a_u1_c, z_a)
            X_a_p_c = X_a_p_c.reshape(X.shape[0], 1)
            X_p_c_all.append(X_a_p_c)
        X_p_c = numpy.hstack(X_p_c_all)
        if not X.shape == X_p_c.shape:
            raise ShapeError("Internal transformation led to object malformation. \
                Original shape: {}, current shape: {}".format(X_discrete.shape, X_p_c.shape))
        #  squared euclidean distance
        pdist_c = euclidean_distances(X=X_p_c, squared=True)
        list_pdist_per_class.append(pdist_c)
    return sum(list_pdist_per_class)


def discretize_columns(X: numpy.ndarray, s: int) -> numpy.ndarray:
    r"""Transform a matrix of continuous into discrete features.

    Parameters
    ----------
    X : numpy.ndarray
        Feature matrix with dimensions (observations, features).
        Note: should only contain continuous features.
    s : int
        Number of categories to group all elements of a feature into.
        Needs to be larger than 0.

    Returns
    -------
    numpy.ndarray
        Matrix of dimensions like original with values representing categories. 

    Notes
    -----
    Discretizing of continuous values see eq. 18, pp.14 :cite:t:`Wilson1997`

    .. math::

        discretize_a(x) = 
        \begin{cases} 
            s, \text{if $x=max_a$, else} \\
            \lfloor (x-min_a)/w_a \rfloor+1 
        \end{cases}

    """
    if not isinstance(s, int):
        t = type(s)
        raise ValueError("s needs to be int, but is {}".format(t))
    if not s > 0:
        raise ValueError("s needs to be larger than 0, but is {}".format(s))

    widths = _get_all_interval_widths(X, s)
    all_cols = []
    for col in range(X.shape[1]):
        z = _discretize_column(
            X[:, col], s, widths[0, col], widths[1, col], widths[2, col])
        all_cols.append(z)
    return numpy.stack(all_cols, 1)


def normalized_diff(X: numpy.ndarray) -> numpy.ndarray:
    r"""Computes Normalised Difference Metric between continuous features

    Parameters
    ----------
    X : numpy.ndarray
        Feature matrix with dimensions (observations, features).

    Returns
    -------
    numpy.ndarray
        Pair-wise distance matrix of dimensions (observations, observations).

    Notes
    -----
    Based on normalized_diff (Equation 13) from :cite:t:`Wilson1997`. 
    As per the paper the square root is not taken, because the individual 
    attribute distances are themselves squared when used in the HVDM function. 
    For overview see pp. 22. 

    .. math:: 
        normalized\_diff(x, y) \\
        &= \sum_{a=1}^{num} normalized\_diff_a(x, y) \\
        &= \sum_{a=1}^{num} \left (\frac {|x_{a} - y_{a}|} {4\sigma_a}\right ) ^2 \\
        & = \sum_{a=1}^{num} \left(\left| \frac {x_{a}} {4\sigma_a} - \frac {y_{a}} {4\sigma_a} \right|\right) ^2

    where `num` is the list of continuous attributes. Corresponds to 
    :math:`\sum_{a=1}^{num} d_{a}^2(x, y)` 
    from :py:func:`smote_likes.distance_metrics.hvdm`.

    """
    x_num_sd = numpy.std(X, axis=0, keepdims=True)
    dispersion = numpy.where(x_num_sd == 0, 1, 4*x_num_sd)

    x_num_normalzied = numpy.divide(X, dispersion)
    x_num_dist = euclidean_distances(
        X=x_num_normalzied,
        squared=True
    )
    return x_num_dist
